{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "- How we can dump large dataset on S3 and read it by using Boto\n",
    "\n",
    "- Learn this by uploading churn dataset on S3, train a Keras DL model by `Churn_Modelling.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S3 + Boto:\n",
    "- pip install awscli (!pip install awscli on Google Colab)\n",
    "- $ aws configure (!aws configure on Google Colab)\n",
    "- AWS Access Key ID [None]: ...\n",
    "- AWS Secret Access Key [None]: ...\n",
    "- Default region name [None]: ...\n",
    "- Default output format [None]: ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "bucket = \"makeschooldata\"\n",
    "file_name = \"data/Churn_Modelling.csv\"\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "# 's3' is a key word. create connection to S3 using default config and all buckets within S3\n",
    "\n",
    "obj = s3.get_object(Bucket=bucket, Key=file_name)\n",
    "# get object and file (key) from bucket\n",
    "\n",
    "df = pd.read_csv(obj['Body']) # 'Body' is a key word\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Churn Prediction\n",
    "\n",
    "- Lets first read: https://medium.com/@pushkarmandot/build-your-first-deep-learning-neural-network-model-using-keras-in-python-a90b5864116d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "X = df.iloc[:, 3:13].values\n",
    "y = df.iloc[:, 13].values\n",
    "\n",
    "print(X)\n",
    "print(X.shape)\n",
    "print(y)\n",
    "\n",
    "label_encoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = label_encoder_X_1.fit_transform(X[:, 1])\n",
    "label_encoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = label_encoder_X_2.fit_transform(X[:, 2])\n",
    "print(X)\n",
    "print(X.shape)\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(categorical_features=[1])\n",
    "X = one_hot_encoder.fit_transform(X).toarray()\n",
    "X = X[:, 1:]\n",
    "# print('M:')\n",
    "# print(X[:, :10])\n",
    "# print(X[:, 10])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Feature Scaling\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "print(X_train.shape)\n",
    "\n",
    "classifier = Sequential()\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(output_dim=6, init='uniform', activation='relu', input_dim=11))\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(output_dim=6, init='uniform', activation='relu'))\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim=1, init='uniform', activation='sigmoid'))\n",
    "# Compiling Neural Network\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# Fitting our model\n",
    "classifier.fit(X_train, y_train, batch_size=10, nb_epoch=50, verbose=1)\n",
    "# Predicting the Test set results\n",
    "y_predict = classifier.predict(X_test)\n",
    "print(y_predict)\n",
    "y_predict = (y_predict > 0.5)\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 as lite\n",
    "\n",
    "con = lite.connect('population.db')\n",
    "\n",
    "with con:\n",
    "    cur = con.cursor()\n",
    "    cur.execute(\"CREATE TABLE Population(id INTEGER PRIMARY KEY, country TEXT, population INT)\")\n",
    "    cur.execute(\"INSERT INTO Population VALUES(NULL,'Germany',81197537)\")\n",
    "    cur.execute(\"INSERT INTO Population VALUES(NULL,'France', 66415161)\")\n",
    "    cur.execute(\"INSERT INTO Population VALUES(NULL,'Spain', 46439864)\")\n",
    "    cur.execute(\"INSERT INTO Population VALUES(NULL,'Italy', 60795612)\")\n",
    "    cur.execute(\"INSERT INTO Population VALUES(NULL,'Spain', 46439864)\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('population.db')\n",
    "query = \"SELECT country FROM Population WHERE population > 50000000;\"\n",
    "\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "for country in df['country']:\n",
    "    print(country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the MongoDB and insert and have query in Python\n",
    "\n",
    "Read: https://marcobonzanini.com/2015/09/07/getting-started-with-mongodb-and-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "\n",
    "client = MongoClient()\n",
    "\n",
    "db = client['tutorial']\n",
    "coll = db['articles']\n",
    "\n",
    "doc = {\n",
    "    \"title\": \"An article about MongoDB and Python\",\n",
    "    \"author\": \"Marco\",\n",
    "    \"publication_date\": datetime.utcnow(),\n",
    "    # more fields\n",
    "}\n",
    "\n",
    "doc_id = coll.insert_one(doc).inserted_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "client = MongoClient()\n",
    "\n",
    "db = client['tutorial']\n",
    "coll = db['articles']\n",
    "\n",
    "for doc in coll.find():\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntaxes:\n",
    "\n",
    "sudo mkdir -p /data/db\n",
    "\n",
    "whoami\n",
    "\n",
    "sudo chown miladtoutounchian /data/db\n",
    "\n",
    "./bin/mongod\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download MongoDB Compass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
